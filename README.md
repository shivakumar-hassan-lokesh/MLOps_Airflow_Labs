Airflow Lab â€“ Wholesale Customers Clustering Pipeline
K-Means Clustering Workflow using Apache Airflow & Docker

This project implements an end-to-end machine learning pipeline using Apache Airflow orchestrated inside Docker containers.
The workflow performs K-Means clustering on the Wholesale Customers Dataset and automates all steps from loading data â†’ preprocessing â†’ model training â†’ elbow method evaluation.

ğŸš€ Project Overview

This lab demonstrates how to:

Use Dockerized Airflow without installing Airflow locally

Build an automated ML pipeline using Airflow DAGs

Load and preprocess data

Train a K-Means clustering model

Compute SSE values and determine the optimal number of clusters (Elbow Method)

Save trained models inside the container

View logs, execution graph, and task outputs in Airflow UI

The DAG runs four sequential tasks:

Load Dataset (load_data_task)

Preprocess Data (data_preprocessing_task)

Build & Save K-Means Model (build_save_model_task)

Load Model & Evaluate with Elbow Method (load_model_task)

ğŸ“‚ Project Structure
LAB_1/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ airflow.cfg
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ Wholesale customers data.csv      # Custom dataset
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ wholesale_model.sav               # Saved K-Means model
â”‚   â”‚
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ lab.py                            # All ML logic (load, preprocess, train, elbow)
â”‚   â”‚
â”‚   â””â”€â”€ airflow.py                             # DAG definition
â”‚
â”œâ”€â”€ logs/                                      # Auto-generated by Airflow
â”œâ”€â”€ plugins/
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ setup.sh
â””â”€â”€ README.md

ğŸ“Š Dataset Used: Wholesale Customers Data

This dataset contains 440 customer records, each describing the annual spending of clients across six product categories.
It is commonly used for clustering, segmentation, and unsupervised ML tasks.

Columns include:

Fresh

Milk

Grocery

Frozen

Detergents_Paper

Delicassen

For this lab, the features were scaled and clustered using MinMaxScaler + K-Means.

âš™ï¸ Pipeline Tasks (What Each Task Does)
1ï¸âƒ£ load_data()

Reads Wholesale customers data.csv

Converts it into a serialized Base64 string for Airflow XCom compatibility

Passes data to the next task

2ï¸âƒ£ data_preprocessing()

Decodes the Base64 data

Removes missing values (if any)

Selects clustering columns

Applies MinMaxScaler()

Returns scaled NumPy array encoded in Base64

3ï¸âƒ£ build_save_model()

Runs KMeans with cluster counts from 1 to 50

Computes SSE values for elbow curve

Saves final model as:

dags/model/wholesale_model.sav

4ï¸âƒ£ load_model_elbow()

Loads the saved .sav model

Uses KneeLocator to find the elbow point

Prints and returns the optimal number of clusters

ğŸ³ Running the Project (Docker + Airflow)
1. Start Airflow for the first time
docker compose up airflow-init

2. Start all Airflow services
docker compose up

3. Access Airflow UI

Open:
ğŸ‘‰ http://localhost:8080

Login:

Username: airflow

Password: airflow

4. Run the DAG

Locate DAG: Airflow_Wholesale_Customers

Click Trigger DAG

Wait for all tasks to turn green

ğŸŸ¢ Results
âœ” The DAG runs successfully
âœ” A K-Means model is trained and saved
âœ” Elbow Method determines the optimal number of clusters

You can see the result in Airflow logs:

Optimal no. of clusters: X


(Where X is the elbow value)

Model file saved:

dags/model/wholesale_model.sav
